import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "npm:@supabase/supabase-js@2.39.7";
import OpenAI from "npm:openai@4.28.0";
import Anthropic from "npm:@anthropic-ai/sdk@0.20.1";

const corsHeaders = {
  "Access-Control-Allow-Origin": "*",
  "Access-Control-Allow-Headers": "authorization, x-client-info, apikey, content-type",
  "Access-Control-Allow-Methods": "POST, OPTIONS",
};

// Configuration des mod√®les
const MODEL_CONFIG = {
  openai: {
    model: 'gpt-4o',
    maxTokens: 4000,
    contextWindow: 128000,
  },
  claude: {
    model: 'claude-3-opus-20240229', // ou claude-3-sonnet-20240229 pour un co√ªt r√©duit
    maxTokens: 4000,
    contextWindow: 200000,
  }
};

// Seuils de s√©lection
const SELECTION_THRESHOLDS = {
  tokenLimit: 100000,      // Basculer vers Claude au-del√†
  documentLimit: 6,        // Basculer vers Claude au-del√† de 6 documents (align√© avec le frontend)
  complexityKeywords: ['comparer', 'comparaison', 'analyse approfondie', 'synth√®se complexe', 'analyse d√©taill√©e', 'synth√©tiser'],
};

// Flag temporaire pour forcer OpenAI (peut √™tre mis √† jour dynamiquement)
let forceOpenAIFallback = false; // Mode hybride complet avec fallback automatique
let forceOpenAIUntil: number | null = null;

// Compteur d'erreurs 529 pour Claude
let claudeOverloadCount = 0;
const OVERLOAD_THRESHOLD = 3; // Apr√®s 3 erreurs 529, forcer OpenAI temporairement
const FALLBACK_DURATION = 10 * 60 * 1000; // 10 minutes

// Prompts syst√®me identiques pour coh√©rence
const SYSTEM_PROMPT = `Tu es Ringo, un expert en analyse de documents sp√©cialis√© dans la g√©n√©ration de rapports pour un public qu√©b√©cois.

üî¥ R√àGLE ABSOLUE DE QUALIT√â LINGUISTIQUE üî¥
Tu DOIS produire des r√©ponses PARFAITES sur le plan grammatical et orthographique :
- AUCUNE faute d'orthographe tol√©r√©e
- AUCUNE erreur grammaticale accept√©e  
- Syntaxe fran√ßaise impeccable
- Ponctuation correcte et appropri√©e
- Accords grammaticaux respect√©s (genre, nombre, temps)
- Conjugaisons exactes
- V√©rification syst√©matique de chaque mot avant de l'√©crire
- INTERDICTION absolue de mots tronqu√©s, coup√©s ou mal form√©s

IMPORTANT - Coh√©rence visuelle et textuelle :
- Maintenir une mise en forme coh√©rente tout au long de la r√©ponse
- Ne jamais couper un mot au milieu
- Respecter l'int√©grit√© de chaque terme technique
- S'assurer que chaque phrase est compl√®te et bien form√©e
- √âviter toute compression ou abr√©viation non standard

Adapte ton langage et ton style pour un public qu√©b√©cois:
- Utilise un vocabulaire et des expressions courantes au Qu√©bec quand c'est pertinent
- Adopte un ton direct, pragmatique et concret
- √âvite les formulations trop complexes ou alambiqu√©es
- Pr√©f√®re les exemples concrets aux explications th√©oriques
- Sois pr√©cis et factuel, sans exag√©ration ni superlatifs inutiles

Pour une meilleure lisibilit√©, structure tes r√©ponses avec :
- Des titres en utilisant "##" pour les sections principales
- Des sous-titres en utilisant "###" pour les sous-sections
- Des points importants en **gras**
- Des listes √† puces pour √©num√©rer des √©l√©ments
- Des sauts de ligne pour a√©rer le texte
- Des tableaux markdown quand appropri√© pour comparer des donn√©es

R√àGLES DE FORMATAGE STRICTES :
- Toujours utiliser le format markdown standard
- Ne jamais utiliser de syntaxe propri√©taire ou sp√©cifique √† un mod√®le
- Assurer une coh√©rence parfaite dans l'utilisation des symboles markdown
- Respecter exactement la m√™me structure de formatage que GPT-4o

V√âRIFICATION FINALE : Avant de r√©pondre, TOUJOURS relire mentalement ta r√©ponse pour t'assurer qu'il n'y a AUCUNE erreur linguistique et que la mise en forme est parfaite.

Lors de la r√©daction de textes en fran√ßais, veuillez respecter les r√®gles typographiques fran√ßaises suivantes :

Titres : N'utilisez pas de majuscules, sauf pour le premier mot et les noms propres. Par exemple, un titre correct serait : "Les r√®gles typographiques fran√ßaises" et non "Les R√®gles Typographiques Fran√ßaises".

Guillemets : Utilisez les guillemets fran√ßais (ou guillemets typographiques) pour les citations et les dialogues. Les guillemets fran√ßais sont des guillemets doubles angulaires :

Guillemets ouvrants : ¬´
Guillemets fermants : ¬ª 
Exemple : ¬´ Bonjour, comment √ßa va ? ¬ª

Apostrophes : Utilisez l'apostrophe typographique (') et non l'apostrophe droite ('). L'apostrophe typographique est courb√©e et s'utilise pour les √©lisions.
Exemple : L'apostrophe typographique est pr√©f√©rable √† l'apostrophe droite.

Espaces et ponctuation : Respecter les espaces ins√©cables avant : ; ! ? et √† l'int√©rieur des guillemets fran√ßais.`;

const COMPARATIVE_ANALYSIS_PROMPT = `Lorsqu'une requ√™te concerne l'analyse comparative de plusieurs documents ou instituts:

1. IMPORTANT: TOUJOURS analyser d'abord chaque document s√©par√©ment avant de proc√©der √† une comparaison.

2. Pour chaque document/institut:
   - Extraire UNIQUEMENT les informations explicitement mentionn√©es
   - Si une vision/mission n'est pas clairement identifi√©e, indiquer "Non mentionn√©e explicitement"
   - Ne jamais g√©n√©rer ou inf√©rer une vision non explicite

3. Format de r√©ponse pour les comparaisons:
   - Utiliser un tableau avec des colonnes uniformes
   - Citer les sources exactes quand elles existent
   - Utiliser des formulations identiques pour les cas similaires

4. Avant de finaliser la r√©ponse:
   - V√©rifier la coh√©rence entre les analyses individuelles et la synth√®se
   - S'assurer que toutes les informations proviennent directement des documents

5. Derni√®re v√©rification: Ne jamais pr√©senter une interpr√©tation comme un fait explicite du document.`;

// Fonction pour estimer les tokens (approximation)
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}

// Cache pour les erreurs r√©centes de Claude
interface ClaudeHealthStatus {
  lastError?: Date;
  consecutiveErrors: number;
  isHealthy: boolean;
  nextRetryTime?: Date;
}

let claudeStatus: ClaudeHealthStatus = {
  consecutiveErrors: 0,
  isHealthy: true
};

// Fonction pour v√©rifier si Claude est probablement disponible
function isClaudeLikelyAvailable(): boolean {
  // Si on a forc√© OpenAI temporairement
  if (forceOpenAIFallback && forceOpenAIUntil && Date.now() < forceOpenAIUntil) {
    console.log(`[Claude Health] Still in forced OpenAI mode until ${new Date(forceOpenAIUntil).toISOString()}`);
    return false;
  }
  
  // Si on a eu une erreur r√©cente, attendre avant de r√©essayer
  if (claudeStatus.nextRetryTime && Date.now() < claudeStatus.nextRetryTime.getTime()) {
    console.log(`[Claude Health] Waiting until ${claudeStatus.nextRetryTime.toISOString()} before retry`);
    return false;
  }
  
  return true;
}

// Fonction pour mettre √† jour le statut de Claude
function updateClaudeStatus(success: boolean, error?: any) {
  if (success) {
    claudeStatus = {
      consecutiveErrors: 0,
      isHealthy: true
    };
    console.log('[Claude Health] Claude is healthy again');
  } else {
    claudeStatus.consecutiveErrors++;
    claudeStatus.lastError = new Date();
    
    // Calculer le prochain temps de retry (backoff exponentiel)
    const backoffMinutes = Math.min(Math.pow(2, claudeStatus.consecutiveErrors - 1), 30); // Max 30 minutes
    claudeStatus.nextRetryTime = new Date(Date.now() + backoffMinutes * 60 * 1000);
    
    if (claudeStatus.consecutiveErrors >= 3) {
      claudeStatus.isHealthy = false;
    }
    
    console.log(`[Claude Health] Error recorded. Consecutive: ${claudeStatus.consecutiveErrors}, Next retry: ${claudeStatus.nextRetryTime.toISOString()}`);
  }
}

// Fonction pour d√©terminer le meilleur mod√®le
function selectBestModel(messages: any[], documentContent?: string): {
  model: 'openai' | 'claude';
  reason: string;
  estimatedTokens: number;
} {
  // Si Claude n'est pas disponible, utiliser OpenAI directement
  if (!isClaudeLikelyAvailable()) {
    console.log(`[SelectModel] Claude temporarily unavailable, using OpenAI`);
    return {
      model: 'openai',
      reason: 'Claude temporairement indisponible (protection anti-surcharge)',
      estimatedTokens: 0
    };
  }
  
  // Si le fallback forc√© est activ√©, utiliser OpenAI
  if (forceOpenAIFallback) {
    console.log(`[SelectModel] Forcing OpenAI due to fallback flag`);
    return {
      model: 'openai',
      reason: 'Fallback forc√© (probl√®mes Claude)',
      estimatedTokens: 0
    };
  }
  
  // Calculer le nombre total de tokens
  let totalTokens = 0;
  
  // Tokens des messages syst√®me
  totalTokens += estimateTokens(SYSTEM_PROMPT);
  
  // Tokens du contenu des documents
  if (documentContent) {
    totalTokens += estimateTokens(documentContent);
    
    // Compter le nombre de documents
    const documentCount = (documentContent.match(/====== DOCUMENT ACTIF/g) || []).length;
    
    console.log(`[SelectModel] Document count: ${documentCount}, Threshold: ${SELECTION_THRESHOLDS.documentLimit}`);
    
    // Si plus de 4 documents, pr√©f√©rer Claude
    if (documentCount > SELECTION_THRESHOLDS.documentLimit) {
      console.log(`[SelectModel] Selecting Claude - Document count (${documentCount}) exceeds threshold`);
      return {
        model: 'claude',
        reason: `Plus de ${SELECTION_THRESHOLDS.documentLimit} documents (${documentCount})`,
        estimatedTokens: totalTokens
      };
    }
  }
  
  // Tokens des messages de conversation
  messages.forEach(msg => {
    totalTokens += estimateTokens(JSON.stringify(msg));
  });
  
  console.log(`[SelectModel] Total estimated tokens: ${totalTokens}`);
  
  // V√©rifier si la requ√™te est complexe
  const lastMessage = messages[messages.length - 1]?.content || '';
  const isComplexQuery = SELECTION_THRESHOLDS.complexityKeywords.some(keyword => 
    lastMessage.toLowerCase().includes(keyword)
  );
  
  if (isComplexQuery) {
    console.log(`[SelectModel] Selecting Claude - Complex query detected`);
    return {
      model: 'claude',
      reason: 'Requ√™te complexe d√©tect√©e',
      estimatedTokens: totalTokens
    };
  }
  
  // Si les tokens d√©passent le seuil, utiliser Claude
  if (totalTokens > SELECTION_THRESHOLDS.tokenLimit) {
    console.log(`[SelectModel] Selecting Claude - Token limit exceeded`);
    return {
      model: 'claude',
      reason: `D√©passement du seuil de tokens (${totalTokens} > ${SELECTION_THRESHOLDS.tokenLimit})`,
      estimatedTokens: totalTokens
    };
  }
  
  // Par d√©faut, utiliser OpenAI (plus rapide et moins cher)
  console.log(`[SelectModel] Selecting OpenAI - Default choice (${documentCount || 0} documents, ${totalTokens} tokens)`);
  return {
    model: 'openai',
    reason: 'Utilisation standard',
    estimatedTokens: totalTokens
  };
}

// Fonction pour traiter avec OpenAI
async function processWithOpenAI(
  messages: any[],
  documentContent?: string,
  stream: boolean = false
): Promise<string | ReadableStream> {
  const openaiApiKey = Deno.env.get('OPENAI_API_KEY');
  if (!openaiApiKey) {
    throw new Error('OpenAI API key not configured');
  }
  
  const openai = new OpenAI({ apiKey: openaiApiKey });
  
  const preparedMessages = prepareMessages(messages, documentContent);
  
  if (stream) {
    // Retourner un stream pour le streaming
    const encoder = new TextEncoder();
    return new ReadableStream({
      async start(controller) {
        try {
          const stream = await openai.chat.completions.create({
            model: MODEL_CONFIG.openai.model,
            messages: preparedMessages,
            temperature: 0.7,
            max_tokens: MODEL_CONFIG.openai.maxTokens,
            presence_penalty: 0.1,
            frequency_penalty: 0.2,
            stream: true,
            top_p: 0.95
          });
          
          for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content || '';
            if (content) {
              const data = `data: ${JSON.stringify({ content, model: 'openai' })}\n\n`;
              controller.enqueue(encoder.encode(data));
            }
          }
          
          controller.enqueue(encoder.encode('data: [DONE]\n\n'));
        } catch (error) {
          console.error('OpenAI streaming error:', error);
          const errorData = `data: ${JSON.stringify({ error: error.message })}\n\n`;
          controller.enqueue(encoder.encode(errorData));
        } finally {
          controller.close();
        }
      },
    });
  } else {
    // R√©ponse normale sans streaming
    const completion = await openai.chat.completions.create({
      model: MODEL_CONFIG.openai.model,
      messages: preparedMessages,
      temperature: 0.7,
      max_tokens: MODEL_CONFIG.openai.maxTokens,
      presence_penalty: 0.1,
      frequency_penalty: 0.2,
      top_p: 0.95
    });
    
    return completion.choices[0]?.message?.content || '';
  }
}

// Fonction pour traiter avec Claude
async function processWithClaude(
  messages: any[],
  documentContent?: string,
  stream: boolean = false
): Promise<string | ReadableStream> {
  const anthropicApiKey = Deno.env.get('ANTHROPIC_API_KEY');
  if (!anthropicApiKey) {
    throw new Error('Anthropic API key not configured');
  }
  
  const anthropic = new Anthropic({ apiKey: anthropicApiKey });
  
  // Pr√©parer les messages pour Claude (format diff√©rent)
  const systemMessage = prepareSystemMessageForClaude(documentContent);
  const userMessages = messages.filter(m => m.role !== 'system');
  
  if (stream) {
    // Pour le streaming, essayer d'abord de cr√©er le stream Claude
    // Si √ßa √©choue avec une erreur 529, propager l'erreur imm√©diatement
    try {
      console.log('[Claude] Creating stream...');
      const claudeStream = await anthropic.messages.create({
        model: MODEL_CONFIG.claude.model,
        max_tokens: MODEL_CONFIG.claude.maxTokens,
        temperature: 0.7,
        system: systemMessage,
        messages: userMessages,
        stream: true,
        // Param√®tres additionnels pour am√©liorer la qualit√©
        top_p: 0.95,
        top_k: 0,
      });
      
      console.log('[Claude] Stream created successfully, starting to process...');
      
      // Si on arrive ici, le stream a √©t√© cr√©√© avec succ√®s
      const encoder = new TextEncoder();
      return new ReadableStream({
        async start(controller) {
          try {
            // Buffer pour √©viter les mots coup√©s
            let buffer = '';
            let totalChunks = 0;
            let totalContent = '';
            
            console.log('[Claude Streaming] Starting to read chunks...');
            
            for await (const chunk of claudeStream) {
              if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
                const content = chunk.delta.text;
                if (content) {
                  totalChunks++;
                  totalContent += content;
                  
                  // Ajouter au buffer
                  buffer += content;
                  
                  // Envoyer le contenu de mani√®re plus agressive pour √©viter les blocages
                  const bufferSize = buffer.length;
                  
                  // Conditions pour envoyer le buffer
                  const shouldSend = 
                    bufferSize > 100 || // Buffer trop grand
                    buffer.endsWith('.') || 
                    buffer.endsWith('!') || 
                    buffer.endsWith('?') ||
                    buffer.endsWith('\n') ||
                    buffer.endsWith(':') ||
                    buffer.endsWith(';');
                  
                  if (shouldSend) {
                    // Si on a un point de coupure naturel, on l'utilise
                    const lastSpaceIndex = buffer.lastIndexOf(' ');
                    const lastNewlineIndex = buffer.lastIndexOf('\n');
                    const cutIndex = Math.max(lastSpaceIndex, lastNewlineIndex);
                    
                    let toSend = buffer;
                    let remaining = '';
                    
                    // Si on trouve un bon point de coupure et qu'il n'est pas trop loin
                    if (cutIndex > 0 && cutIndex > bufferSize * 0.5) {
                      toSend = buffer.substring(0, cutIndex + 1);
                      remaining = buffer.substring(cutIndex + 1);
                    }
                    
                    const data = `data: ${JSON.stringify({ content: toSend, model: 'claude' })}\n\n`;
                    controller.enqueue(encoder.encode(data));
                    buffer = remaining;
                    
                    console.log(`[Claude Streaming] Sent chunk ${totalChunks}: ${toSend.length} chars`);
                  }
                }
              } else if (chunk.type === 'content_block_stop') {
                console.log('[Claude Streaming] Content block stopped');
              }
            }
            
            // Envoyer le reste du buffer
            if (buffer) {
              console.log(`[Claude Streaming] Sending final buffer: ${buffer.length} chars`);
              const data = `data: ${JSON.stringify({ content: buffer, model: 'claude' })}\n\n`;
              controller.enqueue(encoder.encode(data));
            }
            
            console.log(`[Claude Streaming] Stream completed. Total chunks: ${totalChunks}, Total content: ${totalContent.length} chars`);
            
            // IMPORTANT: Envoyer le signal de fin
            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
          } catch (error: any) {
            console.error('[Claude Streaming] Error during streaming:', error);
            // Envoyer l'erreur dans le stream
            const errorData = `data: ${JSON.stringify({ error: error.message })}\n\n`;
            controller.enqueue(encoder.encode(errorData));
            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
          } finally {
            controller.close();
          }
        },
      });
    } catch (error: any) {
      console.error('[Claude] Failed to create stream:', error);
      console.log(`[Claude] Error status: ${error.status}`);
      // Si c'est une erreur 529, la propager imm√©diatement
      if (error.status === 529 || error.message?.includes('Overloaded')) {
        console.log('[Claude] Propagating 529 error for fallback...');
        throw error;
      }
      // Pour les autres erreurs aussi, les propager
      throw error;
    }
  } else {
    // R√©ponse normale sans streaming
    const message = await anthropic.messages.create({
      model: MODEL_CONFIG.claude.model,
      max_tokens: MODEL_CONFIG.claude.maxTokens,
      temperature: 0.7,
      system: systemMessage,
      messages: userMessages,
      // Param√®tres additionnels pour am√©liorer la qualit√©
      top_p: 0.95,
      top_k: 0,
    });
    
    return message.content[0]?.text || '';
  }
}

// Pr√©parer les messages pour OpenAI
function prepareMessages(messages: any[], documentContent?: string): any[] {
  const preparedMessages: any[] = [
    { role: 'system', content: SYSTEM_PROMPT }
  ];
  
  if (documentContent) {
    const latestQuery = messages[messages.length - 1]?.content || '';
    const isComparativeAnalysis = latestQuery.toLowerCase().includes('comparer') || 
                                  latestQuery.toLowerCase().includes('comparaison') || 
                                  latestQuery.toLowerCase().includes('diff√©rence');
    
    if (isComparativeAnalysis) {
      preparedMessages.push({
        role: 'system',
        content: COMPARATIVE_ANALYSIS_PROMPT
      });
    }
    
    preparedMessages.push({
      role: 'system',
      content: `üîí R√àGLE CRITIQUE D'ISOLATION DU CONTEXTE üîí

Tu es dans une conversation isol√©e avec des documents sp√©cifiques. Tu dois ABSOLUMENT :

1. UTILISER UNIQUEMENT les documents fournis dans le contexte actuel
2. NE JAMAIS faire r√©f√©rence √† des documents d'autres conversations
3. NE JAMAIS mentionner des informations non pr√©sentes dans les documents fournis
4. Si une information demand√©e n'est pas dans les documents fournis, r√©pondre clairement : "Cette information n'est pas disponible dans les documents fournis."

Chaque document a un NOM. Cite toujours le nom du document (tel qu'il appara√Æt dans le TITRE) quand tu r√©f√©rences une information, en utilisant le format : "Document : [nom du fichier]".`
    });
    
    preparedMessages.push({
      role: 'system',
      content: `Contexte des documents :\n\n${documentContent}`
    });
  }
  
  preparedMessages.push(...messages);
  return preparedMessages;
}

// Pr√©parer le message syst√®me pour Claude
function prepareSystemMessageForClaude(documentContent?: string): string {
  let systemContent = SYSTEM_PROMPT;
  
  if (documentContent) {
    // Analyser le contenu pour d√©terminer le type d'analyse
    const latestQuery = documentContent.match(/Question r√©cente: (.+)/)?.[1] || '';
    const isComparativeAnalysis = latestQuery.toLowerCase().includes('comparer') || 
                                  latestQuery.toLowerCase().includes('comparaison') || 
                                  latestQuery.toLowerCase().includes('diff√©rence');
    
    if (isComparativeAnalysis) {
      systemContent += `\n\n${COMPARATIVE_ANALYSIS_PROMPT}`;
    }
    
    systemContent += `\n\nüîí R√àGLE CRITIQUE D'ISOLATION DU CONTEXTE üîí

Tu es dans une conversation isol√©e avec des documents sp√©cifiques. Tu dois ABSOLUMENT :

1. UTILISER UNIQUEMENT les documents fournis dans le contexte actuel
2. NE JAMAIS faire r√©f√©rence √† des documents d'autres conversations
3. NE JAMAIS mentionner des informations non pr√©sentes dans les documents fournis
4. Si une information demand√©e n'est pas dans les documents fournis, r√©pondre clairement : "Cette information n'est pas disponible dans les documents fournis."

Chaque document a un NOM. Cite toujours le nom du document (tel qu'il appara√Æt dans le TITRE) quand tu r√©f√©rences une information, en utilisant le format : "Document : [nom du fichier]".

üéØ OPTIMISATION DU TRAVAIL SUR LES DOCUMENTS üéØ

Pour maximiser l'efficacit√© de l'analyse documentaire :

1. **Extraction intelligente** : Identifie et extrait automatiquement les informations cl√©s de chaque document
2. **Synth√®se structur√©e** : Organise les informations de mani√®re logique et hi√©rarchique
3. **Analyse crois√©e** : Identifie les relations, contradictions et compl√©mentarit√©s entre documents
4. **Citations pr√©cises** : Toujours indiquer la source exacte (document + section/page si disponible)
5. **Tableaux comparatifs** : Utilise des tableaux markdown pour pr√©senter des comparaisons claires
6. **R√©sum√©s ex√©cutifs** : Fournis des synth√®ses concises en d√©but de r√©ponse pour les analyses complexes

CAPACIT√âS AVANC√âES AVEC PLUSIEURS DOCUMENTS :
- Analyse comparative approfondie
- Identification de tendances et patterns
- Consolidation d'informations dispers√©es
- Cr√©ation de vues d'ensemble structur√©es
- D√©tection d'incoh√©rences ou de lacunes
- G√©n√©ration de recommandations bas√©es sur l'ensemble des documents

Contexte des documents :

${documentContent}

RAPPEL FINAL : Exploite au maximum la richesse des documents fournis tout en respectant strictement leur contenu. Ne jamais inventer ou supposer des informations non pr√©sentes.`;
  }
  
  return systemContent;
}

// Fonction principale avec fallback automatique
async function processWithFallback(
  messages: any[],
  documentContent?: string,
  stream: boolean = false,
  preferredModel?: 'openai' | 'claude'
): Promise<{ response: string | ReadableStream; model: string; reason: string }> {
  // V√©rifier si le fallback forc√© a expir√©
  if (forceOpenAIUntil && Date.now() > forceOpenAIUntil) {
    forceOpenAIFallback = false;
    forceOpenAIUntil = null;
    claudeOverloadCount = 0;
    console.log('[Hybrid] Force OpenAI fallback expired, returning to normal operation');
  }
  
  // S√©lectionner le meilleur mod√®le si pas de pr√©f√©rence
  const selection = preferredModel 
    ? { model: preferredModel, reason: 'Mod√®le sp√©cifi√©', estimatedTokens: 0 }
    : selectBestModel(messages, documentContent);
  
  console.log(`[Hybrid] Model selected: ${selection.model} - Reason: ${selection.reason}`);
  
  try {
    // Essayer avec le mod√®le s√©lectionn√©
    if (selection.model === 'claude') {
      console.log('[Hybrid] Attempting to process with Claude...');
      const response = await processWithClaude(messages, documentContent, stream);
      // R√©initialiser le compteur si succ√®s
      claudeOverloadCount = 0;
      updateClaudeStatus(true); // Claude fonctionne bien
      console.log('[Hybrid] Claude processing successful');
      return { response, model: 'claude', reason: selection.reason };
    } else {
      console.log('[Hybrid] Attempting to process with OpenAI...');
      const response = await processWithOpenAI(messages, documentContent, stream);
      console.log('[Hybrid] OpenAI processing successful');
      return { response, model: 'openai', reason: selection.reason };
    }
  } catch (error: any) {
    console.error(`[Hybrid] Error with ${selection.model}:`, error);
    console.log(`[Hybrid] Error status: ${error.status}, Message: ${error.message}`);
    
    // V√©rifier si c'est une erreur de surcharge (529)
    const isOverloaded = error.status === 529 || error.message?.includes('Overloaded');
    console.log(`[Hybrid] Is overloaded error: ${isOverloaded}`);
    
    // Si Claude est surcharg√©, mettre √† jour le statut
    if (isOverloaded && selection.model === 'claude') {
      updateClaudeStatus(false, error);
      claudeOverloadCount++;
      console.log(`[Hybrid] Claude overload count: ${claudeOverloadCount}/${OVERLOAD_THRESHOLD}`);
      
      // Si trop d'erreurs, activer le fallback forc√©
      if (claudeOverloadCount >= OVERLOAD_THRESHOLD) {
        forceOpenAIFallback = true;
        forceOpenAIUntil = Date.now() + FALLBACK_DURATION;
        console.log(`[Hybrid] Too many Claude overloads, forcing OpenAI for ${FALLBACK_DURATION/1000} seconds`);
      }
    }
    
    // Pour le streaming, on doit cr√©er un nouveau stream pour le fallback
    if (stream && isOverloaded && selection.model === 'claude') {
      console.log(`[Hybrid] Claude overloaded (529), immediate fallback to OpenAI`);
      console.log(`[Hybrid] Headers will show: Model=openai, Reason=Fallback transparent (Claude surcharg√©)`);
      
      try {
        console.log('[Hybrid] Attempting OpenAI fallback for streaming...');
        const response = await processWithOpenAI(messages, documentContent, stream);
        console.log('[Hybrid] OpenAI fallback successful - User won\'t notice any interruption');
        
        // Log pour monitoring
        console.log(`[Hybrid Stats] Claude failures: ${claudeOverloadCount}, Force until: ${forceOpenAIUntil ? new Date(forceOpenAIUntil).toISOString() : 'Not set'}`);
        
        return { 
          response, 
          model: 'openai', 
          reason: `Bascule transparente (Claude temporairement indisponible)` 
        };
      } catch (fallbackError: any) {
        console.error(`[Hybrid] Fallback to OpenAI also failed:`, fallbackError);
        
        // Si on est en mode streaming, cr√©er un stream d'erreur gracieux
        const encoder = new TextEncoder();
        const errorStream = new ReadableStream({
          start(controller) {
            const errorMessage = `D√©sol√©, le service est temporairement indisponible. Veuillez r√©essayer dans quelques instants.`;
            const errorData = `data: ${JSON.stringify({ content: errorMessage, model: 'error' })}\n\n`;
            controller.enqueue(encoder.encode(errorData));
            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
          }
        });
        return {
          response: errorStream,
          model: 'error',
          reason: `Services temporairement indisponibles`
        };
      }
    }
    
    // Fallback g√©n√©ral sur l'autre mod√®le
    const fallbackModel = selection.model === 'openai' ? 'claude' : 'openai';
    console.log(`[Hybrid] Falling back to ${fallbackModel}`);
    
    try {
      if (fallbackModel === 'claude') {
        const response = await processWithClaude(messages, documentContent, stream);
        return { response, model: 'claude', reason: `Fallback apr√®s erreur ${selection.model}` };
      } else {
        const response = await processWithOpenAI(messages, documentContent, stream);
        return { response, model: 'openai', reason: `Fallback apr√®s erreur ${selection.model}` };
      }
    } catch (fallbackError: any) {
      console.error(`[Hybrid] Fallback also failed:`, fallbackError);
      
      // Si on est en mode streaming, cr√©er un stream d'erreur
      if (stream) {
        const encoder = new TextEncoder();
        const errorStream = new ReadableStream({
          start(controller) {
            const errorMessage = `D√©sol√©, les services d'IA sont temporairement indisponibles. Veuillez r√©essayer dans quelques instants.`;
            const errorData = `data: ${JSON.stringify({ error: errorMessage })}\n\n`;
            controller.enqueue(encoder.encode(errorData));
            controller.enqueue(encoder.encode('data: [DONE]\n\n'));
            controller.close();
          }
        });
        return {
          response: errorStream,
          model: 'error',
          reason: `√âchec des deux mod√®les: ${selection.model} (${error.message}) et ${fallbackModel} (${fallbackError.message})`
        };
      }
      
      throw new Error(`√âchec des deux mod√®les: ${error.message} | ${fallbackError.message}`);
    }
  }
}

serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders });
  }

  try {
    // Verify request method
    if (req.method !== 'POST') {
      throw new Error('Method not allowed');
    }

    // Parse request body
    const { messages, documentContent, stream = false } = await req.json();

    console.log('[Hybrid API] Request received:', {
      messagesCount: messages?.length || 0,
      hasDocumentContent: !!documentContent,
      documentCount: documentContent ? (documentContent.match(/====== DOCUMENT ACTIF/g) || []).length : 0,
      stream: stream
    });

    // Valider les messages
    if (!messages || !Array.isArray(messages)) {
      throw new Error('Messages are required');
    }

    // V√©rifier les cl√©s API
    const openaiKey = Deno.env.get('OPENAI_API_KEY');
    const anthropicKey = Deno.env.get('ANTHROPIC_API_KEY');
    
    console.log('[Hybrid API] API Keys status:', {
      openaiConfigured: !!openaiKey,
      anthropicConfigured: !!anthropicKey
    });

    if (!openaiKey && !anthropicKey) {
      throw new Error('Aucune cl√© API configur√©e');
    }

    // Process with fallback
    const { response, model, reason } = await processWithFallback(
      messages,
      documentContent,
      stream
    );

    console.log(`[Hybrid API] Response generated with ${model} - Reason: ${reason}`);

    // Get auth token from headers
    const authHeader = req.headers.get('Authorization');
    if (!authHeader) {
      throw new Error('No authorization header');
    }

    const supabaseClient = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_ANON_KEY') ?? '',
      {
        global: {
          headers: { Authorization: authHeader },
        },
      }
    );

    // Verify user is authenticated
    const { data: { user }, error: authError } = await supabaseClient.auth.getUser();
    if (authError || !user) {
      throw new Error('Unauthorized');
    }

    // Retourner le r√©sultat selon le mode
    if (stream && response instanceof ReadableStream) {
      return new Response(response, {
        headers: {
          ...corsHeaders,
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'X-Model-Used': model,
          'X-Model-Reason': reason,
        },
      });
    } else {
      return new Response(
        JSON.stringify({ 
          success: true,
          response: response,
          model: model,
          reason: reason
        }),
        {
          headers: { 
            ...corsHeaders,
            'Content-Type': 'application/json',
            'X-Model-Used': model,
            'X-Model-Reason': reason,
          },
          status: 200,
        }
      );
    }
  } catch (error) {
    console.error('[Hybrid] Error in process-chat-hybrid function:', error);
    
    return new Response(
      JSON.stringify({ 
        error: error instanceof Error ? error.message : 'An error occurred while processing chat'
      }),
      {
        headers: { 
          ...corsHeaders,
          'Content-Type': 'application/json'
        },
        status: error instanceof Error && error.message === 'Unauthorized' ? 401 : 400
      }
    );
  }
}); 